# Baseline inference configuration using a public model (for testing)
# Use this if you don't have LLaMA-3 access yet

model:
  name: "mistralai/Mistral-7B-v0.1"  # Public model, no access required
  dtype: "bfloat16"
  max_model_len: 16384

workload:
  # RAG context lengths to test (in tokens)
  context_lengths: [2048, 4096, 8192]
  # User query length (approximate tokens)
  query_length: 50
  # Decode output length (tokens)
  decode_length: 150
  # Number of samples per context length
  num_samples: 5  # Reduced for testing

inference:
  # vLLM engine parameters
  tensor_parallel_size: 1
  gpu_memory_utilization: 0.9
  max_num_seqs: 256
  enable_prefix_caching: false

measurements:
  # Metrics to collect
  metrics:
    - "ttft"  # Time to First Token
    - "decode_throughput"  # tokens/sec
    - "total_latency"  # Total request latency
    - "gpu_memory"  # Peak GPU memory usage
    - "prefill_time"  # Prefill phase time
    - "decode_time"  # Decode phase time

output:
  results_dir: "results"
  format: "jsonl"
  filename: "baseline_results_public.jsonl"

logging:
  log_dir: "logs"
  level: "INFO"
  console: true
  file: true
